{
    "beaker": "2",
    "evaluators": [
        {
            "name": "HTML",
            "plugin": "HTML",
            "view": {
                "cm": {
                    "mode": "htmlmixed"
                }
            }
        },
        {
            "name": "TeX",
            "plugin": "TeX",
            "view": {
                "cm": {
                    "mode": "stex"
                }
            }
        },
        {
            "name": "JavaScript",
            "plugin": "JavaScript",
            "jsSetting2": "",
            "jsSetting1": "",
            "view": {
                "cm": {
                    "mode": "javascript",
                    "background": "#FFE0F0"
                }
            },
            "languageVersion": "ES2015"
        },
        {
            "name": "Scala",
            "plugin": "Scala",
            "imports": "com.twosigma.beaker.NamespaceClient\ncom.twosigma.beaker.BeakerProgressUpdate\ncom.twosigma.beaker.chart.Color\ncom.twosigma.beaker.chart.xychart.*\ncom.twosigma.beaker.chart.xychart.plotitem.*\ncom.twosigma.beaker.chart.legend.*\ncom.twosigma.beaker.chart.Filter\ncom.twosigma.beaker.easyform.*\ncom.twosigma.beaker.easyform.formitem.*\ncom.twosigma.beaker.chart.GradientColor\ncom.twosigma.beaker.chart.categoryplot.*\ncom.twosigma.beaker.chart.categoryplot.plotitem.*\ncom.twosigma.beaker.chart.treemap.*\ncom.twosigma.beaker.chart.treemap.util.*\nnet.sf.jtreemap.swing.*\ncom.twosigma.beaker.chart.histogram.*\ncom.twosigma.beaker.chart.heatmap.HeatMap",
            "view": {
                "cm": {
                    "mode": "text/x-scala"
                }
            },
            "classPath": "/Users/mkegel/bin/spark-1.6.1/assembly/target/scala-2.11/spark-assembly-1.6.1-hadoop2.6.0.jar"
        }
    ],
    "cells": [
        {
            "id": "sectionlL1RfR",
            "type": "section",
            "title": "Spark with Scala",
            "level": 1,
            "evaluatorReader": false,
            "collapsed": false
        },
        {
            "id": "markdownMNyVLe",
            "type": "markdown",
            "body": [
                "Beaker supports using [Spark](http://spark.apache.org/) with its native language, Scala.",
                "",
                "You must use a Spark version compatible with Scala 2.11 (to be compatible with the Scala in Beaker).  We have one prebuilt you can [download](https://s3.amazonaws.com/beaker-distributions/spark-1.5.0-bin-custom-spark.tgz).",
                "Just add `lib/spark-assembly-1.5.0-hadoop2.4.0.jar` to your class path (with the Language Manager) and then try the following demos."
            ],
            "evaluatorReader": false
        },
        {
            "id": "codeWDegFP",
            "type": "code",
            "evaluator": "Scala",
            "input": {
                "body": [
                    "import org.apache.spark.SparkContext",
                    "import org.apache.spark.SparkContext._",
                    "import org.apache.spark.SparkConf",
                    "",
                    "val conf = new SparkConf().setAppName(\"Simple Application\").set(\"spark.ui.enabled\", \"false\")",
                    "val sc = new SparkContext(\"local[4]\", \"Simple Application\", conf)"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "BeakerDisplay",
                    "innertype": "Error",
                    "object": [
                        "org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:",
                        "org.apache.spark.SparkContext.<init>(SparkContext.scala:147)",
                        ".<init>(<console>:52)",
                        ".<clinit>(<console>)",
                        ".$print$lzycompute(<console>:7)",
                        ".$print(<console>:6)",
                        "$print(<console>)",
                        "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                        "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                        "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                        "java.lang.reflect.Method.invoke(Method.java:497)",
                        "scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:773)",
                        "scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)",
                        "scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:627)",
                        "scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:626)",
                        "scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)",
                        "scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)",
                        "scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:626)",
                        "scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:558)",
                        "scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:554)",
                        "com.twosigma.beaker.scala.util.ScalaEvaluatorGlue.evaluate(ScalaEvaluatorGlue.scala:107)",
                        "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2257)",
                        "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2239)",
                        "  at scala.Option.foreach(Option.scala:257)",
                        "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2239)",
                        "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2312)",
                        "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:91)",
                        "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:147)",
                        "  ... 25 elided",
                        ""
                    ]
                },
                "selectedType": "BeakerDisplay",
                "pluginName": "Scala",
                "shellId": "dd8216a7-3e51-4eb4-a33e-a483bf886256",
                "elapsedTime": 774,
                "height": 52
            },
            "evaluatorReader": true,
            "lineCount": 6
        },
        {
            "id": "markdownDxvZC8",
            "type": "markdown",
            "body": [
                "You can then count (in parallel) how many 'a' and 'b' are inside a text file."
            ],
            "evaluatorReader": false
        },
        {
            "id": "code6Vzt6O",
            "type": "code",
            "evaluator": "Scala",
            "input": {
                "body": [
                    "val logFile = \"../../../../../dist/LICENSE\"",
                    "val logData = sc.textFile(logFile, 2).cache()",
                    "val numAs = logData.filter(line => line.contains(\"a\")).count()",
                    "val numBs = logData.filter(line => line.contains(\"b\")).count()",
                    "println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "Results",
                "pluginName": "Scala",
                "shellId": "dd8216a7-3e51-4eb4-a33e-a483bf886256",
                "elapsedTime": 1621,
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "Lines with a: 159, Lines with b: 88\n"
                        }
                    ],
                    "payload": "()"
                },
                "height": 54
            },
            "evaluatorReader": true,
            "lineCount": 5
        },
        {
            "id": "markdownf3UnSb",
            "type": "markdown",
            "body": [
                "Or you can try to approximate $\\pi$."
            ],
            "evaluatorReader": false
        },
        {
            "id": "codefgeZRo",
            "type": "code",
            "evaluator": "Scala",
            "input": {
                "body": [
                    "val NUM_SAMPLES = 10000000",
                    "val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>",
                    "  val x = Math.random()",
                    "  val y = Math.random()",
                    "  if (x*x + y*y < 1) 1 else 0",
                    "}.reduce(_ + _)",
                    "println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES)"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "Pi is roughly 3.1407756\n"
                        }
                    ],
                    "payload": "()"
                },
                "selectedType": "Results",
                "pluginName": "Scala",
                "shellId": "dd8216a7-3e51-4eb4-a33e-a483bf886256",
                "elapsedTime": 4653,
                "height": 54
            },
            "evaluatorReader": true,
            "lineCount": 7
        }
    ],
    "namespace": {},
    "metadata": {
        "publication-id": "560cb7e9-68b4-4659-9863-ce5d6ce93dd4"
    }
}
